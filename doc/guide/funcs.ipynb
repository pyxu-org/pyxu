{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db69cfdf-a558-479c-adb5-82f907d3ade4",
   "metadata": {},
   "source": [
    "# Loss & Regularization Functionals\n",
    "\n",
    "## What is a Functional?\n",
    "\n",
    "A \"functional\" is a special type of function in mathematics and computational science. Unlike typical functions that map from one vector space to another, a functional takes in vectors and maps them to real numbers. Think of it as a way to assign a numerical \"score\" to any given input. This is particularly useful in optimization problems, where the goal is to find the input that minimizes (or maximizes) this score.\n",
    "\n",
    "In the realm of computational imaging, functionals often serve as objective criteria to optimize. These objectives usually consist of two or more parts: a loss term and a regularization term. In Bayesian contexts, the objective functional often signifies the negative log-posterior density, which needs to be either sampled or summarized (e.g. by its moments).\n",
    "\n",
    "## Functionals Hierarchy in Pyxu\n",
    "\n",
    "Pyxu provides a versatile and robust class hierarchy to define various types of functionals. This structure aims to offer maximal flexibility for both elementary and complex computational imaging tasks. Below is an exploration of these different classes, complete with (very simplified) example implementations.\n",
    "\n",
    "> **Important Note**: The base classes described below are designed as abstract base classes. This means they serve as templates or \"blueprints\" for creating specific functionals that suit your project needs. You're not supposed to instantiate them directly. Instead, you have two main options:\n",
    "> \n",
    "> 1. **Subclassing**: Extend these classes to create customized functionals tailored to your problem. \n",
    "> \n",
    "> 2. **Generic Constructor Routine**: Utilize the `from_source`[üîó](../api/operator.interop.html#general) method to define new functionals from their core methods, like `apply()`[üîó](../api/abc/operator.html#pyxu.abc.Map.apply), `grad()`[üîó](../api/abc/operator.html#pyxu.abc.DiffFunc.grad), or `prox()`[üîó](../api/abc/operator.html#pyxu.abc.ProxFunc.prox).\n",
    ">\n",
    "> Additionally, don't forget to explore our comprehensive Reference API. It features pre-implemented versions of many commonly used functionals, serving as both a shortcut for common tasks and a useful learning resource.\n",
    "\n",
    "### `Func`: The Foundation Stone üß±\n",
    "The `Func`[üîó](../api/abc/operator.html#pyxu.abc.Func) class is the base class in the functional hierarchy. Its core method is `apply()`, which computes the value of the functional at a given input.\n",
    "\n",
    "Here's a simplified example, implementing the squared $L_2$ norm:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from pyxu.abc import Func\n",
    "\n",
    "class SquaredL2(Func):\n",
    "    def apply(self, x):\n",
    "        return np.linalg.norm(x) ** 2\n",
    "```\n",
    "\n",
    "This base functional can be adapted into a loss functional using the `asloss()` method:\n",
    "\n",
    "```python\n",
    "# Transforms the functional into a loss\n",
    "l2_loss = SquaredL2().asloss(data_vector)\n",
    "```\n",
    "\n",
    "In this specific example, the `asloss()` method shifts the functional using a given data vector, transforming it into a loss that measures the squared Euclidean distance between the input and the data.\n",
    "\n",
    "> **Special Cases**: The `asloss()` method doesn't always produce a simple shift of the original functional. For instance, when applied to an entropy functional, it transforms it into the relative entropy.\n",
    "\n",
    "\n",
    "### `DiffFunc`: Differentiable Functionals üéØ\n",
    "\n",
    "The `DiffFunc`[üîó](../api/abc/operator.html#pyxu.abc.DiffFunc) class extends `Func` for functionals that have a well-defined gradient. It introduces an additional method, `grad()`, for gradient computation.\n",
    "\n",
    "```python\n",
    "from pyxu.abc import DiffFunc\n",
    "\n",
    "class SquaredL2(DiffFunc):\n",
    "    def apply(self, x):\n",
    "        return np.linalg.norm(x) ** 2\n",
    "\n",
    "    def grad(self, x):\n",
    "        return 2 * x\n",
    "```\n",
    "\n",
    ">üåàTip: With Pyxu, you can use `from_torch`[üîó](../api/operator.interop.html#pyxu.operator.interop.from_torch) or `from_jax`[üîó](../api/operator.interop.html#pyxu.operator.interop.from_jax) to automatically compute the gradient if you have a PyTorch or JAX implementation of your functional.\n",
    "\n",
    "### `ProxFunc`: Proximable Functionals üõ°Ô∏è\n",
    "\n",
    "For functionals with a simple proximal operator, you'll find `ProxFunc`[üîó](../api/abc/operator.html#pyxu.abc.ProxFunc) extremely useful. It offers the `prox()` method, which evaluates the proximal operator of the functional. Here's an example using the $L_1$ norm:\n",
    "\n",
    "```python\n",
    "from pyxu.abc import ProxFunc\n",
    "\n",
    "class L1Norm(ProxFunc):\n",
    "    def apply(self, x):\n",
    "        return np.abs(x).sum()\n",
    "\n",
    "    def prox(self, x, tau):\n",
    "        return np.sign(x) * np.clip(np.abs(x) - tau, 0, None)\n",
    "```\n",
    "\n",
    "#### Moreau Envelope for Smoothing\n",
    "\n",
    "You can also smooth out a proximable functional using the `moreau_envelope()`[üîó](../api/abc/operator.html#pyxu.abc.ProxFunc.moreau_envelope) method. For example, you can smooth the L1 norm to create the [Huber loss function](https://www.wikiwand.com/en/Huber_loss) as follows:\n",
    "\n",
    "```python\n",
    "huber = L1Norm(dim).moreau_envelope(mu=0.1)\n",
    "```\n",
    "\n",
    "#### Demystifying the Proximal Operator üé≠\n",
    "\n",
    "The proximal operator is a powerful tool, especially in the context of nonsmooth optimization. It allows you to iteratively refine a given point in a way that minimizes a functional. It's like asking the algorithm to fine-tune a guess toward the actual optimal solution. \n",
    "\n",
    "For example, consider the indicator function of a convex set. The proximal operator in this context is simply a projection onto that set. Essentially, it pulls any \"off-the-mark\" points back into the permissible set, ensuring they comply with the constraints of your problem.\n",
    "\n",
    "Mathematically, the proximal operator of a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R} \\cup \\{ +\\infty \\}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{prox}_{\\tau f}(x) = \\arg \\min_{u \\in \\mathbb{R}^n} \\left( f(u) + \\frac{1}{2\\tau} \\| u - x \\|^{2} \\right)\n",
    "$$\n",
    "\n",
    "Here, $\\tau > 0$ is a parameter and $\\| \\cdot \\|$ is the Euclidean norm. The term $\\frac{1}{2\\tau} \\| u - x \\|^{2}$ is a regularization term that pulls the solution $u$ closer to $x$. The proximal operator $\\text{prox}_{\\tau f}(x)$ returns the point $u$ that minimizes this expression, essentially finding a compromise between minimizing $f(u)$ and staying close to the original point $x$.\n",
    "\n",
    "This mathematical tool is invaluable for optimization problems, especially those involving nonsmooth or complex functionals. It provides a way to make \"smart\" steps toward the minimum, even when you can't directly calculate the gradient for all points.\n",
    "\n",
    "### Specialized Classes: `ProxDiffFunc`, `LinFunc`, `QuadraticFunc` üé®\n",
    "\n",
    "These classes are for functionals that offer even more, like being both proximable and differentiable (`ProxDiffFunc`[üîó](../api/abc/operator.html#pyxu.abc.ProxDiffFunc)), or being linear (`LinFunc`[üîó](../api/abc/operator.html#pyxu.abc.LinFunc)) or quadratic (`QuadraticFunc`[üîó](../api/abc/operator.html#pyxu.abc.QuadraticFunc)). Quadratic functionals are especially important in primal-dual methods for faster convergence, so do use them when you can! \n",
    "\n",
    "> Note: when dealing with `ProxDiffFunc` instances, it can be hard to decide whether the gradient or proximal operator should be used for optimization purposes. The general rule-of-thumb is to use the gradient as much as possible, as the latter requires more regularity on the objective functional, which can be leveraged by solvers for fatser convergence. \n",
    "\n",
    "### Implicit Functionals: The Undercover Agents üïµÔ∏è‚Äç‚ôÄÔ∏è\n",
    "\n",
    "In some cases, you may not know the functional itself but you might know its proximal operator or gradient (e.g. for plug-and-play or score-based priors). Pyxu lets you define such \"implicit functionals\" as follows:\n",
    "\n",
    "```python\n",
    "from pyxu.abc import ProxFunc\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "class MedianFilterPrior(ProxFunc):\n",
    "    def apply(self, x):\n",
    "        return NotImplemented # apply method not provided\n",
    "        \n",
    "    def prox(self, x, tau):\n",
    "        return median_filter(x, size=5)\n",
    "```\n",
    "\n",
    "\n",
    "## Crafting Custom Loss Functionals through Composition with Forward Operators\n",
    "\n",
    "Inverse problems often involve unknown variables that are related to observable data through a forward operator. In simple terms, a forward operator is like a \"real-world filter\" that transforms your unknown variable, and what you actually observe is the transformed version. In computational imaging, for example, this could represent the blurring of an image. Pyxu makes it easy to handle these intricacies by allowing you to integrate these forward operators directly into your loss functionals.\n",
    "\n",
    "For example, let's say you have a blurred image, represented by the variable $b$. The blurring occurred through a known process‚Äîdescribed by a forward operator $A$‚Äîapplied to the original image $x$. Mathematically, this is:\n",
    "\n",
    "$$\n",
    "b = A x\n",
    "$$\n",
    "\n",
    "The challenge here is to recover $x$ given $b$ and $A$. But why does this matter? Because in real-world applications, you often don't observe $x$ directly. What you have is $b$, and you have to work your way backward to find $x$.\n",
    "\n",
    "### Practical Example: Deblurring through Least-Squares üå†\n",
    "\n",
    "A common strategy to solve this problem is to minimize the least-squares difference between $b$ and $A x$. In Pyxu, you can create a composite loss functional for this exact problem like so:\n",
    "\n",
    "```python\n",
    "from pyxu.operator import SquaredL2Norm\n",
    "\n",
    "loss = SquaredL2Norm(dim=b.size).asloss(b) * A\n",
    "```\n",
    "\n",
    "What did we do here? We took the squared $L_2$ norm, which measures the distance between two vectors, and used the `.asloss()` method to turn it into a loss functional specifically tailored to our blurred image $b$. The `* A` part then integrates the forward operator $A$ into this loss, making sure we're comparing apples to apples‚Äîor in this case, blurred images to blurred images.\n",
    "\n",
    "### The Benefit: Automatic Propagation üöÄ\n",
    "\n",
    "The beauty of this approach is that it streamlines your optimization. The gradient calculations and other relevant methods get automatically updated to incorporate $A$, making your life a lot easier!\n",
    "\n",
    "### The Takeaway: Flexibility and Power üåà\n",
    "\n",
    "This feature adds another layer of flexibility and power to Pyxu, allowing you to tackle a wide range of problems involving different types of transformations and physical processes with ease."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
