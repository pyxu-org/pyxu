{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7273b55f-fc82-42de-9e10-85dce6037112",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Hands-on Example: Computerised Tomography with Pyxu\n",
    "\n",
    "Welcome to this tutorial where we aim to introduce you to the fundamentals of Pyxu through the concrete example of\n",
    "tomographic reconstruction. In this tutorial, we will cover a range of essential concepts and techniques, starting from\n",
    "the basics of defining the forward model, moving on to traditional methods of image reconstruction, and then delving\n",
    "into advanced strategies like model-based regularization techniques and plug-and-play priors.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378eec8-042d-41e1-8111-7e21728dc932",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Tomographic Reconstruction in a Nutshell\n",
    "\n",
    "Tomographic reconstruction is a computational process used in imaging sciences to reconstruct 2D or 3D images from a\n",
    "series of projections captured at different angles. The term \"tomography\" stems from the Greek words \"tomo,\" meaning\n",
    "\"slice\" or \"section,\" and \"graph,\" meaning \"to write\" or \"to record.\" In essence, tomography aims to construct a\n",
    "representation of an object by investigating its internal structure slice-by-slice. This method has wide-ranging\n",
    "applications, from medical imaging like [CT scans](https://www.radiologyinfo.org/en/info.cfm?pg=bodyct) to industrial\n",
    "quality control, [seismology](https://earthquake.usgs.gov/learn/glossary/?term=seismic%20tomography), material science,\n",
    "and [astronomy](https://www.annualreviews.org/doi/full/10.1146/annurev-astro-081817-051819). For a deeper understanding,\n",
    "various [resources](http://www.slaney.org/pct/pct-toc.html) on the mathematical and computational aspects are available.\n",
    "\n",
    "The fundamental principle behind tomographic reconstruction is the Radon transform, maps a function in 2D or 3D space to\n",
    "a set of line integrals. In the context of medical imaging, for example, these line integrals represent the attenuation\n",
    "of X-rays as they pass through a human body. By capturing these attenuations from various angles, it is possible to\n",
    "reconstruct a cross-sectional image of the internal structure.\n",
    "\n",
    "The process typically involves the following steps:\n",
    "\n",
    "1. **Data Acquisition:** Projection data is gathered from different angles, typically by rotating the X-ray source and\n",
    "   detector around the object being imaged.\n",
    "2. **Back-Projection:** The acquired projection data is then 'smeared' back across the imaging field for each angle.\n",
    "3. **Reconstruction:** Various algorithms, such as Filtered Back-Projection (FBP) or iterative methods, are used to\n",
    "   compute the original image from the smeared data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aed0a6fc-dd14-4db2-913e-295945b31513",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<center>    \n",
    "    \n",
    "<img align=\"center\" src=\"../_static/tutorial/tomography_example.gif\" alt=\"Tomography\" width=70%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17856fdb-b751-4a6e-b512-687cebcae4dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## A Non-Ideal Parrallel-beam Tomographic Setup\n",
    "\n",
    "In this tutorial, we explore the challenges of working with a non-idealized parallel-beam tomographic setup that\n",
    "deviates from the assumptions usually made by traditional Radon transform models. The complexities of our setup include:\n",
    "\n",
    "- **Detector Width**: Detectors feature non-negligible widths, causing 'tube integrals' instead of the 'line integrals'\n",
    "  traditionally assumed by the Radon transform. This results in a column-wise 'smearing' effect in the so-called\n",
    "  *sinogram* —a data representation that stacks horizontally 1D projections of an object at various angles.\n",
    "\n",
    "- **X-ray Beam Intensity**: The X-ray beam's intensity tapers towards the edges, causing a similar 'tapering' effect in\n",
    "  the sinogram columns, which complicates the application of the standard Radon transform.\n",
    "\n",
    "- **Probabilistic Element**: There's a chance that detector pixels might turn off during each rotation, introducing\n",
    "  'missing values' in the sinogram.\n",
    "\n",
    "The figure below juxtaposes two types of sinograms for a specific test object, known as a phantom: one represents the\n",
    "idealized sinogram that Radon-based reconstruction algorithms typically assume, and the other illustrates the\n",
    "non-idealized sinogram as captured by the instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0acdc-4fb1-4595-9d39-8e0bf980f946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T15:55:44.870503Z",
     "start_time": "2023-05-14T15:55:44.861666Z"
    },
    "editable": true,
    "hide_input": true,
    "init_cell": true,
    "nbsphinx": "hidden",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage as ski\n",
    "import warnings as warn\n",
    "import time as t\n",
    "import xdesign as xd\n",
    "\n",
    "warn.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = [9, 6]\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams[\"image.cmap\"] = \"inferno\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf139c8-e418-4b27-9719-50b4fbf0e4ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "# Generate phantom with xdesign\n",
    "np.random.seed(seed)\n",
    "p1 = xd.Foam(size_range=[0.1, 0.01], gap=0.025, porosity=0.5)\n",
    "phantom = xd.discrete_phantom(p1, 160) * 10\n",
    "phantom = np.abs(np.pad(phantom, 20))\n",
    "\n",
    "# Generate theoretical and empirical sinogram \n",
    "rng = np.random.default_rng(seed)\n",
    "psnr = 20\n",
    "angles, wsize = 90, 5\n",
    "ideal_sino = ski.transform.radon(phantom, theta=np.linspace(0, 180, angles), circle=True)\n",
    "sino = sp.ndimage.uniform_filter(ideal_sino, [wsize, 0], mode='constant')\n",
    "mask = rng.binomial(1, 0.95, sino.shape)\n",
    "sino *= mask\n",
    "sino *= sp.signal.get_window('hamming', sino.shape[0])[:, None]\n",
    "sigma = np.abs(sino).max() * (10**(-psnr/10))\n",
    "sino += rng.normal(scale=sigma, size=sino.shape)\n",
    "\n",
    "# Plot phantom and sinograms\n",
    "_ = plt.subplot(1,3,1)\n",
    "_ = plt.imshow(phantom)\n",
    "_ = plt.title(\"Phantom\")\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(1,3,2)\n",
    "_ = plt.imshow(ideal_sino, aspect=0.45)\n",
    "_ = plt.title(\"Ideal Sinogram\")\n",
    "_ = plt.axis('off')\n",
    "\n",
    "\n",
    "_ = plt.subplot(1,3,3)\n",
    "_ = plt.imshow(sino, aspect=0.45)\n",
    "_ = plt.title(\"Measured Sinogram\")\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ea6a7c-d07f-4c88-bdfa-7a5a6697c143",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Naive Application of Filtered-Back Projection\n",
    "\n",
    "In this section, we explore what happens when we naively apply the `iradon` function from the `scikit-image` library,\n",
    "which implements the *Filtered Back Projection (FBP)* algorithm, without accounting for the non-idealities inherent to\n",
    "our tomographic setup. Despite the algorithm's effectiveness in idealized scenarios, we observe that our reconstructed\n",
    "image is riddled with severe artifacts.\n",
    "\n",
    "We attempt to mitigate these artifacts by filling in the gaps in the sinogram through matched filtering. However, as can\n",
    "be seen below, this approach proves insufficient for handling the complex distortions caused by the non-ideal conditions\n",
    "in our setup. This section serves as a poignant illustration of the limitations of using off-the-shelf Radon transform\n",
    "methods in scenarios that deviate from their underlying assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d080861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T15:56:02.947005Z",
     "start_time": "2023-05-14T15:56:02.704023Z"
    },
    "editable": true,
    "hide_input": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fbp_ideal = ski.transform.iradon(radon_image=ideal_sino, filter_name=\"hamming\")\n",
    "fbp_measured = ski.transform.iradon(radon_image=sino, filter_name=\"hamming\")\n",
    "sino_fillin = sp.ndimage.uniform_filter(sino, [wsize, 0], mode='constant')\n",
    "fbp_fillin = ski.transform.iradon(radon_image=sino_fillin, filter_name=\"hamming\")\n",
    "\n",
    "_ = plt.subplot(2,3,1)\n",
    "_ = plt.imshow(phantom)\n",
    "_ = plt.title('Phantom')\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(2,3,2)\n",
    "_ = plt.imshow(fbp_ideal)\n",
    "_ = plt.title('FBP (Ideal Sinogram)')\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(2,3,3)\n",
    "_ = plt.imshow(fbp_measured)\n",
    "_ = plt.title('FBP (Measured Sinogram)')\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(2,3,6)\n",
    "_ = plt.imshow(fbp_fillin)\n",
    "_ = plt.title('FBP (Filled-in Measured Sinogram)')\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f3f06",
   "metadata": {
    "editable": true,
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "## Pyxu's Modularity to the Rescue\n",
    "\n",
    "We now turn our attention to how Pyxu addresses the limitations encountered when using `iradon` for reconstruction in\n",
    "non-ideal conditions. Unlike monolithic approaches, Pyxu’s microservice architecture thrives on modularity, allowing us\n",
    "to decompose the data model into a sequence of simple linear operators: the Radon transform ($R$), followed by filtering\n",
    "($F$), masking ($M$), and finally tapering ($T$).\n",
    "\n",
    "$$\\Phi= T \\circ M \\circ F \\circ R$$\n",
    "\n",
    "Here is a code snippet detailing the construction of such an operator with Pyxu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe749f58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T15:56:07.091387Z",
     "start_time": "2023-05-14T15:56:06.127513Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyxu.abc as pxa\n",
    "import pyxu.operator as pxo\n",
    "from pyxu.operator.interop import from_source\n",
    "import skimage.transform as skt\n",
    "\n",
    "# Radon Operator\n",
    "Radon = from_source(cls=pxa.LinOp,\n",
    "                    shape=(sino.size, phantom.size),\n",
    "                    apply=lambda self, arr: skt.radon(arr.reshape(phantom.shape), \n",
    "                                                      theta=np.linspace(0, 180, angles), \n",
    "                                                      circle=True).ravel(),\n",
    "                    adjoint=lambda self, arr: skt.iradon(arr.reshape(sino.shape), \n",
    "                                                         filter_name=None, \n",
    "                                                         circle=True).ravel(),\n",
    "                    vectorize=[\"apply\", \"adjoint\"], vmethod=\"scan\", enforce_precision=[\"apply\", \"adjoint\"])\n",
    "\n",
    "# 1D Filtering\n",
    "boxcar = np.asarray(sp.signal.get_window(\"boxcar\", wsize)); boxcar /= wsize\n",
    "BoxCar1D = pxo.Stencil(kernel=[boxcar, np.array([1.0])], center=(wsize // 2, 0), arg_shape=sino.shape,)\n",
    "\n",
    "# Partial Masking\n",
    "Mask = pxo.DiagonalOp(mask.ravel())\n",
    "\n",
    "# Tapering\n",
    "taper = np.outer(sp.signal.get_window(\"hamming\", sino.shape[0]), np.ones(sino.shape[1]))\n",
    "Taper = pxo.DiagonalOp(taper.ravel())\n",
    "\n",
    "# Compose operators\n",
    "Phi = Taper * Mask * BoxCar1D * Radon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4503bbb-4126-4b36-9260-67b0702f881b",
   "metadata": {},
   "source": [
    "Note that the `apply` and `adjoint` methods of the composite operator $\\Phi$ are automatically inferred by Pyxu,\n",
    "leveraging standard composition rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26922fb-cb9e-4fa4-ba31-ff8be36a2955",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In Pyxu, one of the powerful features is the ability to define matrix-free linear operators, a crucial advantage for\n",
    "large-scale imaging problems.  In layman's terms, you define an operator by just coding its `apply` and `adjoint`\n",
    "methods. This bypasses the need for storing the operator as a cumbersome matrix, which not only saves memory but also\n",
    "boosts computational speed.\n",
    "\n",
    "In the example above, we have for example used the `radon` and `iradon` functions from `scikit-image` to define the\n",
    "`apply` and `adjoint` methods of our Radon operator. This exemplifies Pyxu's core philosophy: You don't have to reinvent\n",
    "the wheel. Feel free to incorporate useful components from other libraries. Pyxu is designed to stitch these disparate\n",
    "elements together into a seamless, modular workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802691b5-78ea-4312-bbb4-18d5a1ba8949",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Internally, Pyxu maintains a detailed record of the composition chains and arithmetic operations that define each\n",
    "operator. This comprehensive tracking is made accessible through the operator's `expr()` method, which can reveal the\n",
    "intricate web of calculations and transformations that the operator encompasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467f595-898f-415a-be11-c748aae23f3d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(Phi.expr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf81b094-ded2-4cd4-8918-9ed08eb4a2af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can now assess the effectiveness of our composite operator, $\\Phi$, in replicating the real-world acquisition setup.\n",
    "By juxtaposing the sinogram predicted by $\\Phi$ with the one actually measured, we find that the two are nearly\n",
    "indistinguishable, aside from minor noise variations. This close resemblance provides strong validation for the accuracy\n",
    "of our custom tomographic system model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45f252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T15:56:07.914281Z",
     "start_time": "2023-05-14T15:56:07.877724Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_sino = Phi(phantom.ravel()).reshape(sino.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0d847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T15:56:09.032838Z",
     "start_time": "2023-05-14T15:56:08.849014Z"
    },
    "editable": true,
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = plt.subplot(1,3,1)\n",
    "_ = plt.imshow(sino, aspect=0.45)\n",
    "_ = plt.title(\"Measured Sinogram\")\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(1,3,2)\n",
    "_ = plt.imshow(predicted_sino, aspect=0.45)\n",
    "_ = plt.title(\"Predicted Sinogram\")\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(1,3,3)\n",
    "_ = plt.imshow(np.abs(sino-predicted_sino), aspect=0.45)\n",
    "_ = plt.title(\"Absolute Difference\")\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43859fcf-65c8-4691-aaa4-9c05036a76a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The real strength here lies in Pyxu's ability to efficiently stitch simple generic operators together to create a\n",
    "coherent and robust data model that accommodates the complexities of our specific tomographic setup. This modular design\n",
    "stands in sharp contrast to adapting a one-size-fits-all routine like `iradon` in `scikit-image`, which would\n",
    "necessitate an unwieldy and complex API to account for all potential non-idealities encountered in practice.\n",
    "\n",
    "By utilizing Pyxu's flexible architecture, we can effortlessly extend or modify each microservice to suit our needs,\n",
    "making it a practical and maintainable solution for handling the intricate challenges of non-ideal image reconstruction\n",
    "setups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0128c9ee-970a-4e59-8e70-1009c7f38031",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Image Reconstruction with Pyxu\n",
    "\n",
    "### Pseudoinverse Solution\n",
    "\n",
    "Now that we've accurately modeled our acquisition system using the composite operator $\\Phi$, we can proceed to the\n",
    "reconstruction phase. Traditional methods like filtered backprojection are no longer applicable here, due to the\n",
    "complexities introduced in our custom setup. Consequently, we must resort to generic solvers, such as the conjugate\n",
    "gradient method, to solve the corresponding (dampened) normal equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beadd20d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T21:14:54.647410Z",
     "start_time": "2023-05-12T21:14:54.642117Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img align=\"center\" src=\"../_static/tutorial/g15273.png\" alt=\"Forward model\" width=70%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988dd6d6-dc87-4036-8f4a-be438025462f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In Pyxu, this task is greatly simplified by the `pinv` (pseudoinverse) method provided by `LinOp` instances. The `pinv`\n",
    "method allows us to implement simple reconstruction techniques without getting lost in the details. \n",
    "\n",
    "The following code snippet illustrates its application in our tomographic setting. For the sake of this demonstration,\n",
    "we employ a custom stopping criterion (relative error smaller than $10^{-3}$ and 500 iterations tops) and incorporate a\n",
    "dampening factor of 4 to improve the algorithm's resilience to noise and numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc7cf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T15:56:13.429529Z",
     "start_time": "2023-05-14T15:56:12.378063Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyxu.opt.stop as pxs\n",
    "\n",
    "default_stop_crit = pxs.RelError(eps=1e-3, var=\"x\", f=None, norm=2, satisfy_all=True) | pxs.MaxIter(500)\n",
    "pinv_solution = Phi.pinv(sino.ravel(), damp=4,  \n",
    "                         kwargs_init=dict(show_progress=False, verbosity=50),\n",
    "                         kwargs_fit=dict(stop_crit=default_stop_crit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13b4ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T15:56:15.543205Z",
     "start_time": "2023-05-14T15:56:15.329470Z"
    },
    "editable": true,
    "hide_input": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = plt.subplot(1,3,1)\n",
    "_ = plt.imshow(phantom)\n",
    "_ = plt.title('Phantom')\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(1,3,2)\n",
    "_ = plt.imshow(fbp_fillin)\n",
    "_ = plt.title('Filtered Back Projection')\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(1,3,3)\n",
    "_ = plt.imshow(pinv_solution.reshape(phantom.shape))\n",
    "_ = plt.title('Pseudoinverse Solution')\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1b82b-ccab-4ca0-a985-0cf96770c9d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The results are truly impressive! In a little more than a second of (laptop) computing time and with only minimal\n",
    "coding, we've substantially improved various facets of image quality, including resolution, contrast, and artifact\n",
    "removal.\n",
    "\n",
    "The secret to this success lies in Pyxu's modular architecture. It enabled us to efficiently address the specific\n",
    "complexities of our custom tomographic system, allowing for high-quality, reliable image reconstructions —even when\n",
    "using straightforward algorithms like the conjugate gradient method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af419c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T21:32:49.651522Z",
     "start_time": "2023-05-12T21:32:49.648030Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Bayesian Inversion\n",
    "\n",
    "In this section, we explore a more advanced approach to image reconstruction using Bayesian models. These models offer a\n",
    "rigorous yet adaptable framework to manage uncertainties and to incorporate prior information. In the Bayesian context,\n",
    "the 'likelihood' quantifies how well our model explains the observed data, while the 'prior' encodes pre-existing\n",
    "beliefs or assumptions about the solution.\n",
    "\n",
    "<center>\n",
    "\n",
    "<img align=\"center\" src=\"../_static/tutorial/g15005.png\" alt=\"Forward model\" width=70%>\n",
    "</center>\n",
    "\n",
    "Specifically, we employ a Bayesian model with a least-squares log-likelihood, and couple it with a composite prior that\n",
    "includes terms for total variation, sparsity, and positivity. The corresponding (negative) log-posterior distribution is\n",
    "given by:\n",
    "\n",
    "$$\n",
    "-\\log p(f | g) \\propto \\frac{1}{2\\sigma^2}\\|g - \\Phi(f)\\|_2^2 +  \\lambda\\|\\nabla f\\|_{1} + \\mu \\|f\\|_1 + \\iota(f\\geq 0).\n",
    "$$\n",
    "\n",
    "In this equation, the total variation term  $\\lambda\\|\\nabla f\\|_1$ aims to produce solutions that feature minimal\n",
    "abrupt changes between adjacent pixels, which helps in preserving edges and reducing noise. The sparsity term $\\mu\n",
    "\\|f\\|_1$ encourages solutions that are primarily zero or close to zero, thereby focusing on the essential features in\n",
    "the image. Lastly, the positivity constraint $f\\geq 0$ assures that the reconstructed image is physically meaningful,\n",
    "which is especially critical in areas like medical imaging where negative values would not be interpretable.\n",
    "\n",
    "Unlike pseudoinverse methods, Bayesian inversion does not produce a single estimate but instead yields an *ensemble* of\n",
    "possible solutions, each with varying degrees of credibility as characterized by the posterior distribution. One can\n",
    "then either sample from this distribution or summarize it through its moments or modes. The mode of the posterior, known\n",
    "as *Maximum A Posteriori (MAP)* estimation, provides the most credible image given the observed data, data model, and\n",
    "prior. In our particular setup, the MAP estimate is defined as:\n",
    "\n",
    "$$\n",
    "\\hat{f} \\in \\arg\\min_{f\\geq 0} \\frac{1}{2\\sigma^2}\\|g - \\Phi(f)\\|_2^2 +  \\lambda\\|\\nabla f\\|_{1} + \\mu \\|f\\|_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ede5f-e415-4cb3-92b7-2799e191d8f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In Pyxu, the MAP can be obtained as follows (note that, for simplicity, we smooth the TV term): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239ac106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:02:36.820973Z",
     "start_time": "2023-05-14T16:02:32.972735Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyxu.operator import Gradient, SquaredL2Norm, L1Norm, PositiveOrthant, PositiveL1Norm\n",
    "\n",
    "# TV prior\n",
    "grad = Gradient(arg_shape=phantom.shape, accuracy=4, mode='constant', diff_method='fd')\n",
    "lambda_= 0.05 / (2 * sigma**2)\n",
    "huber_norm = L1Norm(grad.shape[0]).moreau_envelope(0.01) #We smooth the L1 norm to facilitate optimisation\n",
    "tv_prior = lambda_ * huber_norm * grad\n",
    "\n",
    "# Positivity + L1 norm \n",
    "posL1 = 0.05 * PositiveL1Norm(phantom.size)\n",
    "\n",
    "# Loss\n",
    "loss = (1/ (2 * sigma**2)) * SquaredL2Norm(dim=sino.size).asloss(sino.ravel()) * Phi\n",
    "\n",
    "# Smooth part of the posterior\n",
    "smooth_posterior = loss + tv_prior\n",
    "smooth_posterior.diff_lipschitz = smooth_posterior.estimate_diff_lipschitz(method=\"svd\", tol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c39655-8afc-4063-9e89-3a38e8b62e72",
   "metadata": {},
   "source": [
    "The code can be broken down as follows: \n",
    "\n",
    "**Defining the Total Variation (TV) Prior**\n",
    "```python\n",
    "# TV prior\n",
    "grad = Gradient(arg_shape=phantom.shape, accuracy=4, mode='constant', diff_method='fd')\n",
    "lambda_= 0.05 / (2 * sigma**2)\n",
    "huber_norm = L1Norm(grad.shape[0]).moreau_envelope(0.01)\n",
    "tv_prior = lambda_ * huber_norm * grad\n",
    "```\n",
    "- `Gradient`: Computes the gradient of an image. The argument `arg_shape=phantom.shape` specifies the shape of the\n",
    "  image, i.e., `phantom.shape`.\n",
    "- `lambda_`: Scales the total variation term. It is calculated based on the noise level, `sigma`, in the observations.\n",
    "- `moreau_envelope(0.01)`: Applies the Moreau envelope to smooth the L1 norm of the TV term, facilitating the downstream\n",
    "  MAP estimation process.\n",
    "\n",
    "**Defining the Positivity and L1 Norm Constraints**\n",
    "```python\n",
    "# Positivity + L1 norm \n",
    "posL1 = 0.05 * PositiveL1Norm(phantom.size)\n",
    "```\n",
    "- `PositiveL1Norm`: This operator combines a sparsity-promoting $L_1$ norm and a positivity constraint.\n",
    "\n",
    "**Defining the Data Likelihood (Loss)**\n",
    "```python\n",
    "# Loss\n",
    "loss = (1/ (2 * sigma**2)) * SquaredL2Norm(dim=sino.size).asloss(sino.ravel()) * Phi\n",
    "```\n",
    "- `SquaredL2Norm`: Represents the squared L2 norm of the residual between the observed and modeled data, scaled by the\n",
    "  noise level, `sigma`.\n",
    "- `.asloss(sino.ravel())`: Sets the observed sinogram as the data.\n",
    "- `* Phi`: Compose the $L_2$ norm with $\\Phi$, the operator modeling the acquisition process.\n",
    "\n",
    "**Defining the Smooth Part of the Posterior**\n",
    "```python\n",
    "# Posterior\n",
    "smooth_posterior = loss + tv_prior\n",
    "beta = smooth_posterior.estimate_diff_lipschitz(method=\"svd\", tol=0.1)\n",
    "```\n",
    "- `smooth_posterior`: The smooth part of the posterior that combines both the likelihood (loss) and the smoothed\n",
    "  `tv_prior`. Will be useful later on.\n",
    "- The method `estimate_diff_lipschitz` allows to compute the Lipschitz constant of the gradient of the differentiable\n",
    "  part of the posterior, that will be used later on to optimally set the step size of the gradient-descent based\n",
    "  optimizer. This step can be time consuming for large scale problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970fe0ea-4370-4470-bda7-36426e9b976e",
   "metadata": {},
   "source": [
    "We are now ready to solve the MAP estimation problem. We do that via proximal gradient descent, a generic solver for\n",
    "problems of the form\n",
    "\n",
    "$$\n",
    "       {\\min_{\\mathbf{x}\\in\\mathbb{R}^N} \\;\\mathcal{F}(\\mathbf{x})\\;\\;+\\;\\;\\mathcal{G}(\\mathbf{x})},\n",
    "$$\n",
    "\n",
    "where $\\mathcal{F}$ is *convex* and *smooth* and $\\mathcal{G}$ is nonsmooth but admits a simple proximity operator.  For\n",
    "our particular setup, we choose $F(f)=\\frac{1}{2\\sigma^2}\\|g - \\Phi(f)\\|_2^2 +  \\lambda\\|\\nabla f\\|_{H}$ and $G(f)= \\mu\n",
    "\\|f\\|_1 + \\iota(f\\geq 0)$, (with $\\lambda\\|\\nabla f\\|_{H}$ denoting the smoothed TV prior). \n",
    "\n",
    "In Pyxu, this is achieved as follows (note that we do not set the solver's step size, this is automatically computed by Pyxu): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011db6f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:02:55.092972Z",
     "start_time": "2023-05-14T16:02:39.064256Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyxu.opt.solver import PGD\n",
    "\n",
    "# Define the solver \n",
    "solver = PGD(f=smooth_posterior, g=posL1, show_progress=False, verbosity=250)\n",
    "\n",
    "# Call fit to trigger the solver\n",
    "solver.fit(x0=0*pinv_solution.ravel(), acceleration=True, stop_crit=default_stop_crit)\n",
    "recon_tv = solver.solution().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef492b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:02:58.430341Z",
     "start_time": "2023-05-14T16:02:58.208405Z"
    },
    "editable": true,
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = plt.subplot(1,3,1)\n",
    "_ = plt.imshow(fbp_fillin)\n",
    "_ = plt.title('Filtered Back Projection')\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(1,3,2)\n",
    "_ = plt.imshow(pinv_solution.reshape(phantom.shape))\n",
    "_ = plt.title('Pseudoinverse Solution')\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(1,3,3)\n",
    "_ = plt.imshow(recon_tv.reshape(phantom.shape))\n",
    "_ = plt.title('MAP Solution (TV+)')\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20519c04-b67e-4573-a171-75e3bdd54aba",
   "metadata": {},
   "source": [
    "Again, the benefits in terms of accuracy when compared to both the pseudoinverse and the naive `iradon` reconstruction\n",
    "routine are substantial. However, it's important to note that this enhanced accuracy was not achieved without some\n",
    "additional effort. Setting up the Bayesian inversion model demands a more intricate configuration compared to simpler\n",
    "methods. Thankfully, the Pyxu library significantly simplifies this process by providing a streamlined framework,\n",
    "thereby alleviating most of the associated complexities.\n",
    "\n",
    "Furthermore, Pyxu offers built-in routines for fine-tuning the hyperparameters of Bayesian models —specifically the\n",
    "regularization parameters $\\lambda$ and $\\mu$ in our case— which can further enhance the performance of the\n",
    "reconstruction. Finally, for those interested in a more nuanced understanding of the solution space, Pyxu includes\n",
    "Markov Chain Monte Carlo (MCMC) samplers. These allow for sampling from the posterior distribution and enable the\n",
    "calculation of various statistical moments, providing a more comprehensive characterization of the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d26af-8417-4f04-9edd-8e663b20b462",
   "metadata": {},
   "source": [
    "### Plug-and-Play Reconstruction Schemes\n",
    "\n",
    "We conclude this tutorial with a simple Plug-and-Play (PnP) reconstruction scheme. The latter offer a modern, versatile\n",
    "approach to image reconstruction that seamlessly combines data fidelity and prior knowledge. These schemes alternate\n",
    "between gradient steps, which aim to keep the solution consistent with observed data, and denoising steps, which\n",
    "introduce prior information to regularize the solution.\n",
    "\n",
    "#### PnP Priors: Implicit and Intuitive\n",
    "\n",
    "One of the unique characteristics of PnP schemes is the use of implicit priors. Instead of using an explicitly defined\n",
    "mathematical function to model the prior, a denoising algorithm is used. This innovative step replaces the proximity\n",
    "operator commonly used in classical optimization with a denoiser, thereby expanding the range of prior models that can\n",
    "be incorporated into the reconstruction process. This implicitness allows for great flexibility and can accommodate more\n",
    "complex structures in the data, which might not be easily expressible in mathematical terms.\n",
    "\n",
    "The key intuition behind PnP is simple yet powerful: if a denoising algorithm can effectively remove noise from an\n",
    "image, then it likely encapsulates some underlying \"truth\" or \"structure\" of the image, which can serve as a good prior.\n",
    "Essentially, the denoising algorithm embodies our prior beliefs about what the solution should look like.\n",
    "\n",
    "In real-world applications, a variety of denoising algorithms have found their way into PnP frameworks, each offering\n",
    "its own set of advantages and limitations. Here are some commonly used denoisers that can act as implicit priors in PnP\n",
    "schemes:\n",
    "\n",
    "- **BM3D (Block-Matching and 3D Filtering)**: A highly effective denoiser, popular in medical imaging. It\n",
    "  collaboratively filters 3D data blocks.\n",
    "  \n",
    "- **Non-Local Means**: Averages similar pixels from the entire image, effectively preserving textures.\n",
    "  \n",
    "- **Wavelet-based Denoising**: Uses wavelet transforms for multi-resolution analysis, isolating and removing noise.\n",
    "\n",
    "- **Deep Learning-based Denoisers**: Algorithms like DnCNN adapt to various noise types and often outperform traditional\n",
    "  methods.\n",
    "\n",
    "- **Total Variation Denoising**: Minimizes the total variation of the image, effectively removing noise while keeping\n",
    "  edges sharp.\n",
    "\n",
    "\n",
    "#### Basic Equations\n",
    "PnP methods are typically found in two forms: proximal or differentiable. \n",
    "\n",
    "In its proximal form, a typical PnP algorithm can be represented by:\n",
    "\n",
    "$$\n",
    "f^{k+1} = \\text{Denoiser}_{\\sigma}(f^k - \\tau \\nabla F(f^k)),\n",
    "$$\n",
    "where $\\nabla F(x)$ is the gradient term ensuring data fidelity.\n",
    "\n",
    "In differential form, influenced by the regularization by denoising (RED) framework, the equation can be expressed as:\n",
    "\n",
    "$$\n",
    "f^{k+1} = f^k - \\tau \\left[\\nabla F(f^k) + \\lambda(f^k- \\text{Denoiser}_{\\sigma}(f^k))\\right],\n",
    "$$\n",
    "where $\\lambda$ encodes the PnP prior strength. \n",
    "\n",
    "> **_NOTE:_** Tweedie's formula tells us that when $\\text{Denoiser}_{\\sigma}$ is an MMSE Gaussian denoiser, we should\n",
    "> choose $\\lambda = 1/\\sigma$ (where $\\sigma$ is the noise level). The term $\\frac{1}{\\sigma}(f^k-\n",
    "> \\text{Denoiser}_{\\sigma}(f^k))$ can then be understood as the    approximate score (gradient) of the distribution of\n",
    "> natural images convolved with a Gaussian with standard deviation $\\sigma$. \n",
    "\n",
    "By understanding and applying these principles, one can utilize PnP schemes to achieve high-quality image\n",
    "reconstructions that are both data-consistent and regularized, all while benefiting from the computational and modeling\n",
    "advantages that come with implicit priors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69adcacb-298b-47ff-af12-a5e60f84d879",
   "metadata": {},
   "source": [
    "In Pyxu, implicit PnP priors can be implemented in both forms via the `ProxDiffFunc` base class. Here is a simple\n",
    "example with a median filter denoiser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4fe071-00e9-4452-9060-527acc698ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter\n",
    "\n",
    "def denoiser(arr, arg_shape, wsize):\n",
    "    arr = arr.reshape(arg_shape)\n",
    "    return median_filter(arr, size=wsize).ravel()\n",
    "\n",
    "class MedianFilterPnP(pxa.ProxDiffFunc):\n",
    "    def __init__(self, arg_shape, wsize):\n",
    "        super().__init__((1,np.prod(arg_shape)))\n",
    "        self._arg_shape = arg_shape\n",
    "        self.wsize= wsize\n",
    "        self.diff_lipschitz = 1\n",
    "    \n",
    "    def apply(self, arr):\n",
    "        return NotImplemented # PnP priors are implicit, they do not have an apply method! \n",
    "    \n",
    "    def grad(self, arr): \n",
    "        return arr - denoiser(arr, self._arg_shape, self.wsize) # Differential form \n",
    "\n",
    "    def prox(self, arr, tau=None): # Parameter tau is not used since this is not a proper proximal operator.\n",
    "        return denoiser(arr, self._arg_shape, self.wsize) # Proximal form    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058217b9-0c9e-4d44-bcfd-3d4c0de8c419",
   "metadata": {},
   "source": [
    "> Note that this is a simplified implementation for example purposes (an actual implmentation would need to abide by\n",
    "> Pyxu's API rules, namely support for batch dimensions, module-agnosticity and precision control).\n",
    "\n",
    "We can now initialize the PnP prior: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d8cc2c-af2a-44c2-a657-ec6dbad5de95",
   "metadata": {},
   "outputs": [],
   "source": [
    "medpnp = MedianFilterPnP(phantom.shape, (3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa60985-8539-47b6-a0b4-08f47af5e0be",
   "metadata": {},
   "source": [
    "For the data consistency term, we simply use the least-squares loss from before. To allow for PGD to tune its gradient\n",
    "step automatically, we need to compute the Lipschtiz constant of the derivative of the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9150a3d1-565d-4cdb-bba5-09c3744cdc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.diff_lipschitz = loss.estimate_diff_lipschitz(method=\"svd\", tol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea96005-2ee5-417f-b511-81045408da2c",
   "metadata": {},
   "source": [
    "To use the PnP prior in proximal form, we feed it to the nonsmooth proximal term `g` of PGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844d0f0-d364-436a-a28f-b9526f7fcc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the solver \n",
    "solver = PGD(f=loss, g=medpnp, show_progress=False, verbosity=75)\n",
    "\n",
    "# Call fit to trigger the solver\n",
    "solver.fit(x0=pinv_solution.ravel(), acceleration=True, stop_crit=pxs.MaxIter(250))\n",
    "recon_pnp_prox = solver.solution().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46cd933-d6e0-4d5a-bfac-fb88c7206d14",
   "metadata": {},
   "source": [
    "> Note that we changed the stopping criterion to a maximum number of iterations, since the fixed-point convergence of\n",
    "> PnP methods is not guaranteed in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1478a8-88a3-4e39-aa3c-c54a236e033a",
   "metadata": {},
   "source": [
    "To use the PnP prior in differential form, we first sum it with the loss and feed the sum to the smooth term `f` of PGD,\n",
    "leaving the nonsmooth term empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e209412-ef79-427d-a91d-591fd7945510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new composite smooth term (sum of the loss and weighted implicit prior) \n",
    "composite_smooth = loss + 0.1 * medpnp\n",
    "\n",
    "# Define the solver \n",
    "solver = PGD(f=composite_smooth, g=None, show_progress=False, verbosity=75)\n",
    "\n",
    "# Call fit to trigger the solver\n",
    "solver.fit(x0=pinv_solution.ravel(), acceleration=True, stop_crit=pxs.MaxIter(250))\n",
    "recon_pnp_smooth = solver.solution().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67fd86b-0694-4cf3-8f03-db5cf6434f16",
   "metadata": {},
   "source": [
    "The results are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7d502-159b-41e3-9930-0cf981757952",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.subplot(2,3,1)\n",
    "_ = plt.imshow(pinv_solution.reshape(phantom.shape))\n",
    "_ = plt.title('Pseudoinverse Solution')\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(2,3,2)\n",
    "_ = plt.imshow(recon_tv.reshape(phantom.shape))\n",
    "_ = plt.title('MAP Solution (TV+)')\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(2,3,3)\n",
    "_ = plt.imshow(recon_pnp_prox.reshape(phantom.shape))\n",
    "_ = plt.title('PnP (Prox)')\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(2,3,6)\n",
    "_ = plt.imshow(recon_pnp_smooth.reshape(phantom.shape))\n",
    "_ = plt.title('PnP (Diff)')\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48db506-1b08-44f5-921b-9c85c46c71a7",
   "metadata": {},
   "source": [
    "Not bad! Despite using a very simple PnP prior, we managed to significantly improve the reconstruction quality w.r.t.\n",
    "the pseudoinverse solution. With respect to the MAP solution, we observe a slight improvement in terms of sharpness and\n",
    "contrast. There is however, not much difference between the PnP priors in differential or proximal form. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac07bf5-8938-430d-9627-c549340de9cd",
   "metadata": {},
   "source": [
    "Note however that PnP priors are harder to fine-tune than Bayesian methods. In particular, the size of the median filter\n",
    "used above is arbitrary, and changing it can significantly change the estimate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ac5ea-b522-4251-8c28-76fbecd3e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "medpnp = MedianFilterPnP(phantom.shape, (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15166cf9-779e-434a-8fd3-36206e3a1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the solver \n",
    "solver = PGD(f=loss, g=medpnp, show_progress=False, verbosity=75)\n",
    "\n",
    "# Call fit to trigger the solver\n",
    "solver.fit(x0=pinv_solution.ravel(), acceleration=True, stop_crit=pxs.MaxIter(250))\n",
    "recon_pnp_prox = solver.solution().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3afa56-33bf-4a3b-8bdc-b3befe4bec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.subplot(1,3,1)\n",
    "_ = plt.imshow(pinv_solution.reshape(phantom.shape))\n",
    "_ = plt.title('Pseudoinverse Solution')\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(1,3,2)\n",
    "_ = plt.imshow(recon_tv.reshape(phantom.shape))\n",
    "_ = plt.title('MAP Solution (TV+)')\n",
    "_ = plt.axis('off')\n",
    "\n",
    "_ = plt.subplot(1,3,3)\n",
    "_ = plt.imshow(recon_pnp_prox.reshape(phantom.shape))\n",
    "_ = plt.title('PnP (Prox)')\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da418c27-88a4-4124-b810-d7f2e6f51cdb",
   "metadata": {},
   "source": [
    "The resulting image looks \"sketchy\" and extremely quantized. Lots of details are lost w.r.t the MAP solution (only the\n",
    "big dark circle subsist). "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "rise": {
   "footer": "Matthieu Simeoni, EPFL Hub for Advanced Image Reconstruction (AIR)",
   "progress": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
